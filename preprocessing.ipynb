{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Wikipedia data\n",
    "This notebook is a workflow for pre-processing the wikipedia data. This includes cleaning the data before it is used further. Shortening the texts based on some criteria and finally saving the data in a format that can be used by the labeling program and training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Beheerder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import preprocessor\n",
    "from preprocessor import Preprocessor\n",
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "ROOT_DIR = preprocessor.ROOT_DIR\n",
    "DATA_PATH = preprocessor.DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing and filtering the UNESCO WORLD HERITAGE sites\n",
    "### 1.1 Subset 1: Spacy embedding filtering\n",
    "#### 1.1.1 Importing the UNESCO data\n",
    "The first subset of the entire Wikipedia dataset will consists of articles for which we are positive that they contain UNESCO World Heritage Sites\n",
    "We start he preprocessing with retrieving the unesco world heritage sites from a dataset and filtering these sites in the large wikipedia dataset. \n",
    "\n",
    "The data is read from a csv file, followed by making a set of all the names in this column with english names. This is done for further processing using the set of names in stead of a dataframe.\n",
    "\n",
    "To see what kind of data we are using, a head of the dataframe is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landmark_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cultural Landscape and Archaeological Remains ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Minaret and Archaeological Remains of Jam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Historic Centres of Berat and Gjirokastra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Butrint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Al Qal'a of Beni Hammad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       landmark_name\n",
       "0  Cultural Landscape and Archaeological Remains ...\n",
       "1          Minaret and Archaeological Remains of Jam\n",
       "2         Historic Centres of Berat and Gjirokastra \n",
       "3                                            Butrint\n",
       "4                            Al Qal'a of Beni Hammad"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(ROOT_DIR)\n",
    "\n",
    "df_unesco = pd.read_csv(os.path.join(DATA_PATH, \"unesco_names.csv\"), header=0, names=[\"landmark_name\"])\n",
    "landmark_names = set(df_unesco[\"landmark_name\"].to_list())\n",
    "\n",
    "df_unesco.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Embedding the UNESCO data\n",
    "For filtering of the wikipedia pages based on the landmark names, the landmark names need to be embedded such that we can do a similarity search. This is needed since the titels of the wikipedia pages are rarely exactly like the landmark names described in the UNESCO dataset.\n",
    "\n",
    "For embedding the names, we are using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embedded landmarks:  1157\n"
     ]
    }
   ],
   "source": [
    "landmark_embeddings = []\n",
    "\n",
    "for landmark_name in landmark_names:\n",
    "    landmark_embeddings.append(preprocessor.ner_spacy(landmark_name))\n",
    "\n",
    "print(\"Number of embedded landmarks: \", len(landmark_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Subsetting the Wikipedia Dataset\n",
    "Comparisson with embeddings takes a lot of time and is hence not feasible for the entire Wikipedia dataset. Hence, we take a subset of the dataset by filtering on Wikipedia articles that occur in (parts of) the UNESCO landmarks list. This is done using the `process_folders` function, which can either be used to filter on title or on landmark names. Based on the variable `title_based`, it decides whether to run `process_file_regex` or `process_file_nlp` respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100 - Started processing 'wiki_00' in folder 'AA'\n",
      "2/100 - Started processing 'wiki_01' in folder 'AA'\n",
      "3/100 - Started processing 'wiki_02' in folder 'AA'\n",
      "4/100 - Started processing 'wiki_03' in folder 'AA'\n",
      "5/100 - Started processing 'wiki_04' in folder 'AA'\n",
      "6/100 - Started processing 'wiki_05' in folder 'AA'\n",
      "7/100 - Started processing 'wiki_06' in folder 'AA'\n",
      "8/100 - Started processing 'wiki_07' in folder 'AA'\n",
      "9/100 - Started processing 'wiki_08' in folder 'AA'\n",
      "10/100 - Started processing 'wiki_09' in folder 'AA'\n",
      "11/100 - Started processing 'wiki_10' in folder 'AA'\n",
      "12/100 - Started processing 'wiki_11' in folder 'AA'\n",
      "13/100 - Started processing 'wiki_12' in folder 'AA'\n",
      "14/100 - Started processing 'wiki_13' in folder 'AA'\n",
      "15/100 - Started processing 'wiki_14' in folder 'AA'\n",
      "16/100 - Started processing 'wiki_15' in folder 'AA'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Beheerder\\Documents\\Prive\\University\\Year2\\Q1\\Text Mining\\assignment\\Text-Mining\\preprocessing.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Beheerder/Documents/Prive/University/Year2/Q1/Text%20Mining/assignment/Text-Mining/preprocessing.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Title for title filtering, trailing space is important for filtering\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Beheerder/Documents/Prive/University/Year2/Q1/Text%20Mining/assignment/Text-Mining/preprocessing.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m title \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUNESCO World Heritage Site \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Beheerder/Documents/Prive/University/Year2/Q1/Text%20Mining/assignment/Text-Mining/preprocessing.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m page_list \u001b[39m=\u001b[39m preprocessor1\u001b[39m.\u001b[39;49mprocess_folders(folders \u001b[39m=\u001b[39;49m folders, landmarks \u001b[39m=\u001b[39;49m landmark_names, debug \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, title \u001b[39m=\u001b[39;49m title, title_based \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Beheerder\\Documents\\Prive\\University\\Year2\\Q1\\Text Mining\\assignment\\Text-Mining\\preprocessor.py:168\u001b[0m, in \u001b[0;36mPreprocessor.process_folders\u001b[1;34m(self, folders, debug, title, title_based, landmarks, datadir)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mfor\u001b[39;00m file_nr, filename \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(os\u001b[39m.\u001b[39mlistdir(folder_path)):\n\u001b[0;32m    166\u001b[0m     file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, filename)\n\u001b[1;32m--> 168\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_file_regex(file_path, title_based, title, landmarks)\n\u001b[0;32m    170\u001b[0m     \u001b[39mif\u001b[39;00m debug:\n\u001b[0;32m    171\u001b[0m         \u001b[39mprint\u001b[39m(\n\u001b[0;32m    172\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfile_nr\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_files\u001b[39m}\u001b[39;00m\u001b[39m - Started processing \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m in folder \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfolder\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Beheerder\\Documents\\Prive\\University\\Year2\\Q1\\Text Mining\\assignment\\Text-Mining\\preprocessor.py:128\u001b[0m, in \u001b[0;36mPreprocessor.process_file_regex\u001b[1;34m(self, file_path, title_based, title, landmarks)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_file_regex\u001b[39m(\u001b[39mself\u001b[39m, file_path, title_based, title, landmarks):\n\u001b[0;32m    126\u001b[0m     \u001b[39m# Load the JSON data\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m--> 128\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file:\n\u001b[0;32m    129\u001b[0m             info_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(line)\n\u001b[0;32m    130\u001b[0m             \u001b[39mif\u001b[39;00m title_based:\n",
      "File \u001b[1;32mc:\\Users\\Beheerder\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folders = [\"AA\", \"AB\"]\n",
    "# Title for title filtering, trailing space is important for filtering\n",
    "title = \"UNESCO World Heritage Site \"\n",
    "\n",
    "page_list = preprocessor.process_folders(folders = folders, landmarks = landmark_names, debug = True, title = title, title_based = False)\n",
    "preprocessor.clear_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 Export Wikipedia subset\n",
    "The resulting subset has 5160 entries, which is more feasible for Spacy embedding filtering. The next step is to save this dataset to a JSON file which can be used later on in the workflow. This is done using the `writeFile` function of the `Preprocessor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.writeFile(page_list, f\"unesco_first_subset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5 Embedding filtering on the Wikipedia subset\n",
    "The next step is to process the subset by filtering based on title and UNESCO landmarks similarity. All article titles are embedded just like the landmarks. A cosine similarity comparrison between the embeddings then indicates whether to include the article. This is done if the similarity is at least `0.97`. The function which does this is `process_file_nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DATA_PATH, \"unesco_first_subset.json\")\n",
    "results = preprocessor.process_file_nlp(file_path, landmark_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.6 Exporting filtered data\n",
    "To be able to use the texts for further cleaning, the current filtered data is being stored as a json file to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.writeFile(results, f\"unesco_wikipedia_pages.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Filtering the Wikipedia dataset\n",
    "The second subset is filtered using the `process_file_title` function, which is called in the `process_folders` function if the parameter `title_based` is set to True. The `process_file_title` function checks whether the string \"UNESCO World Heritage Site \" (trailing space to prevent plural versions) occurs in the article text. If so, the article is included in the subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"AA\", \"AB\"]\n",
    "# Title for title filtering, trailing space is important for filtering\n",
    "title = \"UNESCO World Heritage Site \"\n",
    "\n",
    "page_list = preprocessor.process_folders(folders = folders, landmarks = landmark_names, debug = True, title = title, title_based = True)\n",
    "preprocessor.clear_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Exporting the second subset\n",
    "The final step for the second subset is to export it to a JSON file so that it can be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.writeFile(page_list, f\"unesco_wikipedia_titles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clearning the wikipedia pages\n",
    "\n",
    "### 2.1 Shortening the wikipedia pages\n",
    "For the main purpose of the task, namely converting information from text to a knowledge graph, most important information of a heritage site is stored in the first few paragraphs. Therefore, we will first shorten all the texts to the first 2 paragraphs and with a minimum of 500 words (average paragraph length is 250 words).\n",
    "\n",
    "We start of by defining a function that can shorten a text to 2 paragraphs. This is done by splitting the text on the newline character and then joining the first two paragraphs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unesco_wikipedia_pages = preprocessor.loadFile(f\"unesco_wikipedia_pages.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in unesco_wikipedia_pages:\n",
    "    splitted_text = page[\"text\"].split(\"\\n\")\n",
    "\n",
    "    # Get the lengths of every split and get the first index of the split where the total number is higher than 500\n",
    "    total_length = 0\n",
    "    for index, split in enumerate(splitted_text):\n",
    "        total_length += len(split)\n",
    "        if total_length > 500:\n",
    "            page[\"text\"] = ''.join(splitted_text[:index+1])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the unesco_wikipedia_pages are split per new line. The average paragraph length is 250 words. Therefore, we will shorten the texts with a minimum of 500 words.\n",
    "\n",
    "### 2.2 Removing unwanted characters and fixing unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data with ftfy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1996.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data with given regex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "unesco_wikipedia_pages_clean = preprocessor.fix_unicode(unesco_wikipedia_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the cleaned text is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2750841',\n",
       " 'revid': '182902',\n",
       " 'url': 'https://en.wikipedia.org/wiki?curid=2750841',\n",
       " 'title': 'Alejandro de Humboldt National Park',\n",
       " 'text': 'Alejandro de Humboldt National Park is a national park in the Cuban provinces of Holguín and Guantánamo It is named after the German scientist Alexander von Humboldt who visited the island in 1800 and 1801 The park was inscribed as a UNESCO World Heritage Site in 2001 for of its size altitude range complex lithology landform diversity and wealth of endemic flora and fauna Geography The rivers that flow off the peaks of the park are some of the largest in the insular Caribbean The park is said to be the most humid place in Cuba and this causes a high biological diversity The park has an area of of which land area and marine area Elevation ranges from sea level to on El Toldo Peak',\n",
       " 'original_text': 'Alejandro de Humboldt National Park () is a national park in the Cuban provinces of Holguín and Guantánamo. It is named after the German scientist Alexander von Humboldt who visited the island in 1800 and 1801. The park was inscribed as a UNESCO World Heritage Site in 2001 for of its size, altitude range, complex lithology, landform diversity, and wealth of endemic flora and fauna.Geography.The rivers that flow off the peaks of the park are some of the largest in the insular Caribbean. The park is said to be the most humid place in Cuba and this causes a high biological diversity. The park has an area of , of which land area and marine area. Elevation ranges from sea level to on \"El Toldo\" Peak.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unesco_wikipedia_pages_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Export the cleaned data in separate json files\n",
    "There is a export needed of separate json file for the annotation program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.save_file(unesco_wikipedia_pages_clean, \"subset_texts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text-Mining-xR8YyNgY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
