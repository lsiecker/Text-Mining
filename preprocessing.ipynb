{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Wikipedia data\n",
    "This notebook is a workflow for pre-processing the wikipedia data. This includes cleaning the data before it is used further. Shortening the texts based on some criteria and finally saving the data in a format that can be used by the labeling program and training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\niels\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import preprocessor\n",
    "from preprocessor import Preprocessor\n",
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "ROOT_DIR = preprocessor.ROOT_DIR\n",
    "DATA_PATH = preprocessor.DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing and filtering the UNESCO WORLD HERITAGE sites\n",
    "### 1.1 Subset 1: Spacy embedding filtering\n",
    "#### 1.1.1 Importing the UNESCO data\n",
    "The first subset of the entire Wikipedia dataset will consists of articles for which we are positive that they contain UNESCO World Heritage Sites\n",
    "We start he preprocessing with retrieving the unesco world heritage sites from a dataset and filtering these sites in the large wikipedia dataset. \n",
    "\n",
    "The data is read from a csv file, followed by making a set of all the names in this column with english names. This is done for further processing using the set of names in stead of a dataframe.\n",
    "\n",
    "To see what kind of data we are using, a head of the dataframe is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landmark_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cultural Landscape and Archaeological Remains ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Minaret and Archaeological Remains of Jam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Historic Centres of Berat and Gjirokastra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Butrint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Al Qal'a of Beni Hammad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       landmark_name\n",
       "0  Cultural Landscape and Archaeological Remains ...\n",
       "1          Minaret and Archaeological Remains of Jam\n",
       "2         Historic Centres of Berat and Gjirokastra \n",
       "3                                            Butrint\n",
       "4                            Al Qal'a of Beni Hammad"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor1 = Preprocessor(ROOT_DIR)\n",
    "\n",
    "df_unesco = pd.read_csv(os.path.join(DATA_PATH, \"unesco_names.csv\"), header=0, names=[\"landmark_name\"])\n",
    "landmark_names = set(df_unesco[\"landmark_name\"].to_list())\n",
    "\n",
    "df_unesco.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Embedding the UNESCO data\n",
    "For filtering of the wikipedia pages based on the landmark names, the landmark names need to be embedded such that we can do a similarity search. This is needed since the titels of the wikipedia pages are rarely exactly like the landmark names described in the UNESCO dataset.\n",
    "\n",
    "For embedding the names, we are using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embedded landmarks:  1157\n"
     ]
    }
   ],
   "source": [
    "landmark_embeddings = []\n",
    "\n",
    "for landmark_name in landmark_names:\n",
    "    landmark_embeddings.append(preprocessor1.ner_spacy(landmark_name))\n",
    "\n",
    "print(\"Number of embedded landmarks: \", len(landmark_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Subsetting the Wikipedia Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data\\\\AA'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Niels\\Documents\\University\\Msc\\Year 2\\Q1\\Text Mining\\assignment\\Text-Mining\\preprocessing.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Niels/Documents/University/Msc/Year%202/Q1/Text%20Mining/assignment/Text-Mining/preprocessing.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m page_list \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Niels/Documents/University/Msc/Year%202/Q1/Text%20Mining/assignment/Text-Mining/preprocessing.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m folder \u001b[39min\u001b[39;00m folders:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Niels/Documents/University/Msc/Year%202/Q1/Text%20Mining/assignment/Text-Mining/preprocessing.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     page_list\u001b[39m.\u001b[39mappend(preprocessor1\u001b[39m.\u001b[39;49mprocess_folder(folder \u001b[39m=\u001b[39;49m folder, landmarks \u001b[39m=\u001b[39;49m landmark_names, debug \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, title \u001b[39m=\u001b[39;49m title, title_based \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m))\n",
      "File \u001b[1;32md:\\Niels\\Documents\\University\\Msc\\Year 2\\Q1\\Text Mining\\assignment\\Text-Mining\\preprocessor.py:162\u001b[0m, in \u001b[0;36mPreprocessor.process_folder\u001b[1;34m(self, folder, debug, title, title_based, landmarks, datadir)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[39mProcess all files in a folder in a specific directory. Threads are used to speed up the process.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[39mEvery file is processed in a separate thread.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39m:return: A list of the shared pages\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m folder_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(datadir, folder)\n\u001b[1;32m--> 162\u001b[0m num_files \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(folder_path))\n\u001b[0;32m    164\u001b[0m \u001b[39mfor\u001b[39;00m file_nr, filename \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(os\u001b[39m.\u001b[39mlistdir(folder_path)):\n\u001b[0;32m    165\u001b[0m     file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data\\\\AA'"
     ]
    }
   ],
   "source": [
    "folders = [\"AA\", \"AB\"]\n",
    "# Title for title filtering, trailing space is important for filtering\n",
    "title = \"UNESCO World Heritage Site \"\n",
    "page_list = []\n",
    "\n",
    "for folder in folders:\n",
    "    page_list.append(preprocessor1.process_folder(folder = folder, landmarks = landmark_names, debug = False, title = title, title_based = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 Export Wikipedia subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor1.writeFile(page_list, f\"unesco_first_subset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5 Embedding filtering on the Wikipedia subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DATA_PATH, \"unesco_first_subset.json\")\n",
    "results = preprocessor1.process_file_nlp(file_path, landmark_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.6 Exporting filtered data\n",
    "To be able to use the texts for further cleaning, the current filtered data is being stored as a json file to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor1.writeFile(results, f\"unesco_wikipedia_pages.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Subset 2: String matching filtering\n",
    "#### 1.2.1 Importing the UNESCO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor2 = Preprocessor(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Filtering the Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"AA\", \"AB\"]\n",
    "# Title for title filtering, trailing space is important for filtering\n",
    "title = \"UNESCO World Heritage Site \"\n",
    "page_list = []\n",
    "\n",
    "for folder in folders:\n",
    "    page_list.append(preprocessor.process_folder(folder = folder, landmarks = landmark_names, debug = False, title = title, title_based = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every wikipedia text has the following raw format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Exporting the second subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor2.writeFile(page_list, f\"unesco_wikipedia_titles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clearning the wikipedia pages\n",
    "\n",
    "### 2.1 Shortening the wikipedia pages\n",
    "For the main purpose of the task, namely converting information from text to a knowledge graph, most important information of a heritage site is stored in the first few paragraphs. Therefore, we will first shorten all the texts to the first 2 paragraphs and with a minimum of 500 words (average paragraph length is 250 words).\n",
    "\n",
    "We start of by defining a function that can shorten a text to 2 paragraphs. This is done by splitting the text on the newline character and then joining the first two paragraphs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unesco_wikipedia_pages = preprocessor.loadFile(f\"unesco_wikipedia_pages.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in unesco_wikipedia_pages:\n",
    "    splitted_text = page[\"text\"].split(\"\\n\")\n",
    "\n",
    "    # Get the lengths of every split and get the first index of the split where the total number is higher than 500\n",
    "    total_length = 0\n",
    "    for index, split in enumerate(splitted_text):\n",
    "        total_length += len(split)\n",
    "        if total_length > 500:\n",
    "            page[\"text\"] = ''.join(splitted_text[:index+1])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the unesco_wikipedia_pages are split per new line. The average paragraph length is 250 words. Therefore, we will shorten the texts with a minimum of 500 words.\n",
    "\n",
    "### 2.2 Removing unwanted characters and fixing unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data with ftfy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1996.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data with given regex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "unesco_wikipedia_pages_clean = preprocessor.fix_unicode(unesco_wikipedia_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the cleaned text is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2750841',\n",
       " 'revid': '182902',\n",
       " 'url': 'https://en.wikipedia.org/wiki?curid=2750841',\n",
       " 'title': 'Alejandro de Humboldt National Park',\n",
       " 'text': 'Alejandro de Humboldt National Park is a national park in the Cuban provinces of Holguín and Guantánamo It is named after the German scientist Alexander von Humboldt who visited the island in 1800 and 1801 The park was inscribed as a UNESCO World Heritage Site in 2001 for of its size altitude range complex lithology landform diversity and wealth of endemic flora and fauna Geography The rivers that flow off the peaks of the park are some of the largest in the insular Caribbean The park is said to be the most humid place in Cuba and this causes a high biological diversity The park has an area of of which land area and marine area Elevation ranges from sea level to on El Toldo Peak',\n",
       " 'original_text': 'Alejandro de Humboldt National Park () is a national park in the Cuban provinces of Holguín and Guantánamo. It is named after the German scientist Alexander von Humboldt who visited the island in 1800 and 1801. The park was inscribed as a UNESCO World Heritage Site in 2001 for of its size, altitude range, complex lithology, landform diversity, and wealth of endemic flora and fauna.Geography.The rivers that flow off the peaks of the park are some of the largest in the insular Caribbean. The park is said to be the most humid place in Cuba and this causes a high biological diversity. The park has an area of , of which land area and marine area. Elevation ranges from sea level to on \"El Toldo\" Peak.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unesco_wikipedia_pages_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Export the cleaned data in separate json files\n",
    "There is a export needed of separate json file for the annotation program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.save_file(unesco_wikipedia_pages_clean, \"subset_texts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text-Mining-xR8YyNgY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
